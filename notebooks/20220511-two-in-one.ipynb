{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import afqinsight.nn.tf_models as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from afqinsight.datasets import AFQDataset\n",
    "from afqinsight.nn.tf_models import cnn_lenet, mlp4, cnn_vgg, lstm1v0, lstm1, lstm2, blstm1, blstm2, lstm_fcn, cnn_resnet\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os.path\n",
    "# Harmonization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurocombat_sklearn import CombatModel\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, resample\n",
    "from afqinsight.augmentation import jitter, time_warp, scaling\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "afq_dataset = AFQDataset.from_files(\n",
    "    fn_nodes=\"../data/raw/combined_tract_profiles.csv\",\n",
    "    fn_subjects=\"../data/raw/participants_updated_id.csv\",\n",
    "    dwi_metrics=[\"dki_fa\", \"dki_md\", \"dki_mk\"],\n",
    "    index_col=\"subject_id\",\n",
    "    target_cols=[\"age\", \"dl_qc_score\", \"scan_site_id\"],\n",
    "    label_encode_cols=[\"scan_site_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09423835",
   "metadata": {},
   "outputs": [],
   "source": [
    "afq_dataset.drop_target_na()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(afq_dataset.subjects))\n",
    "print(afq_dataset.X.shape)\n",
    "print(afq_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = list(afq_dataset.as_tensorflow_dataset().as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8837759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([xx[0][None] for xx in full_dataset], 0)\n",
    "y = np.array([yy[1][0] for yy in full_dataset])\n",
    "qc = np.array([yy[1][1] for yy in full_dataset])\n",
    "site = np.array([yy[1][2] for yy in full_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[qc>0]\n",
    "y = y[qc>0]\n",
    "site = site[qc>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1139ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "# EarlyStopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    mode=\"min\",\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97651175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_this(X, y, rounds=2): \n",
    "    new_X = X[:]\n",
    "    new_y = y[:]\n",
    "    for f in range(rounds): \n",
    "        aug_X = np.zeros_like(X)\n",
    "        # Do each channel separately:\n",
    "        for channel in range(aug_X.shape[-1]):\n",
    "            this_X = X[..., channel][..., np.newaxis]\n",
    "            this_X = jitter(this_X, sigma=np.mean(this_X)/25)\n",
    "            this_X = scaling(this_X, sigma=np.mean(this_X)/25)\n",
    "            this_X = time_warp(this_X, sigma=np.mean(this_X)/25)\n",
    "            aug_X[..., channel] = this_X[...,0]\n",
    "        new_X = np.concatenate([new_X, aug_X])\n",
    "        new_y = np.concatenate([new_y, y])\n",
    "    return new_X, new_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "def impute(X_data):\n",
    "    X_data = np.concatenate([imputer.fit_transform(X_data[..., ii])[:, :, None] for ii in range(X_data.shape[-1])], -1)\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-in-one test\n",
    "def cross_site(model_name, name_str, lr, site_1, site_2, site_3, X, y):\n",
    "    # Split the data by sites\n",
    "    X_1 = X[site==site_1]\n",
    "    y_1 = y[site==site_1]\n",
    "    X_2 = X[site==site_2]\n",
    "    y_2 = y[site==site_2]\n",
    "    X_3 = X[site==site_3]\n",
    "    y_3 = y[site==site_3]\n",
    "    # Split the data into train and test sets:\n",
    "    X_train1, X_test, y_train1, y_test = train_test_split(X_1, y_1, test_size=0.2)\n",
    "    X_train2, _, y_train2, _ = train_test_split(X_2, y_2, test_size=0.2)\n",
    "    X_train3, _, y_train3, _ = train_test_split(X_3, y_3, test_size=0.2)\n",
    "    # Imputation\n",
    "    X_train1 = impute(X_train1)\n",
    "    X_train2 = impute(X_train2)\n",
    "    X_train3 = impute(X_train3)\n",
    "    X_test = impute(X_test)\n",
    "    \n",
    "    # Single_site\n",
    "    # Training on site 1\n",
    "    model1 = model_name(input_shape=(100, 72), n_classes=1, output_activation=None, verbose=True)\n",
    "    model1.compile(loss='mean_squared_error',\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                   metrics=['mean_squared_error', \n",
    "                            tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                            'mean_absolute_error'])\n",
    "               \n",
    "    ckpt_filepath1 = tempfile.NamedTemporaryFile().name + '.h5'\n",
    "    ckpt1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = ckpt_filepath1,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    )\n",
    "\n",
    "    log1 = tf.keras.callbacks.CSVLogger(filename=(name_str + '1.csv'), append=True)\n",
    "    callbacks1 = [early_stopping, ckpt1, reduce_lr, log1]\n",
    "    model1.fit(X_train1, y_train1, epochs=n_epochs, batch_size=128,\n",
    "               validation_split=0.2, callbacks=callbacks1)\n",
    "    model1.load_weights(ckpt_filepath1)\n",
    "    \n",
    "    # Training on site 2\n",
    "    model2 = model_name(input_shape=(100, 72), n_classes=1, output_activation=None, verbose=True)\n",
    "    model2.compile(loss='mean_squared_error',\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                   metrics=['mean_squared_error', \n",
    "                            tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                            'mean_absolute_error'])\n",
    "               \n",
    "    ckpt_filepath2 = tempfile.NamedTemporaryFile().name + '.h5'\n",
    "    ckpt2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = ckpt_filepath2,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    )\n",
    "\n",
    "    log2 = tf.keras.callbacks.CSVLogger(filename=(name_str + '2.csv'), append=True)\n",
    "    callbacks2 = [early_stopping, ckpt2, reduce_lr, log2]\n",
    "    model2.fit(X_train2, y_train2, epochs=n_epochs, batch_size=128,\n",
    "               validation_split=0.2, callbacks=callbacks2)\n",
    "    model2.load_weights(ckpt_filepath2)\n",
    "               \n",
    "    # Training on site 3\n",
    "    model3 = model_name(input_shape=(100, 72), n_classes=1, output_activation=None, verbose=True)\n",
    "    model3.compile(loss='mean_squared_error',\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                   metrics=['mean_squared_error', \n",
    "                            tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                            'mean_absolute_error'])\n",
    "\n",
    "    ckpt_filepath3 = tempfile.NamedTemporaryFile().name + '.h5'\n",
    "    ckpt3 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = ckpt_filepath3,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    )\n",
    "    \n",
    "    log3 = tf.keras.callbacks.CSVLogger(filename=(name_str + '3.csv'), append=True)\n",
    "    callbacks3 = [early_stopping, ckpt3, reduce_lr, log3]\n",
    "    model3.fit(X_train3, y_train3, epochs=n_epochs, batch_size=128,\n",
    "               validation_split=0.2, callbacks=callbacks3)\n",
    "    model3.load_weights(ckpt_filepath3)\n",
    "    \n",
    "    # Double cross site\n",
    "    # Training on site 2 and 3\n",
    "    sample = y_test.shape[0]//2\n",
    "    sample1 = resample(X_train2, y_train2, n_samples=sample, replace=False)\n",
    "    sample2 = resample(X_train3, y_train3, n_samples=sample, replace=False)\n",
    "    X_train4 = np.concatenate((sample1[0], sample2[0]), axis=0)\n",
    "    y_train4 = np.concatenate((sample1[1], sample2[1]), axis=0)\n",
    "               \n",
    "    X_train4, y_train4 = shuffle(X_train4, y_train4)\n",
    "    X_train4, y_train4 = augment_this(X_train4, y_train4)\n",
    "    X_train4, y_train4 = shuffle(X_train4, y_train4)\n",
    "    \n",
    "    model4 = model_name(input_shape=(100, 72), n_classes=1, output_activation=None, verbose=True)\n",
    "    model4.compile(loss='mean_squared_error',\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                   metrics=['mean_squared_error', \n",
    "                            tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                            'mean_absolute_error'])\n",
    "    \n",
    "    ckpt_filepath4 = tempfile.NamedTemporaryFile().name + '.h5'\n",
    "    ckpt4 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = ckpt_filepath4,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    )\n",
    "    \n",
    "    log4 = tf.keras.callbacks.CSVLogger(filename=(name_str + '4.csv'), append=True)\n",
    "    callbacks4 = [early_stopping, ckpt4, reduce_lr, log4]\n",
    "    model4.fit(X_train4, y_train4, epochs=n_epochs, batch_size=128,\n",
    "               validation_split=0.2, callbacks=callbacks4)\n",
    "    model4.load_weights(ckpt_filepath4)\n",
    "               \n",
    "    # Testing on site 1\n",
    "    y_predict1 = model1.predict(X_test)\n",
    "    y_predict1 = y_predict1.reshape(y_test.shape)\n",
    "    y_predict2 = model2.predict(X_test)\n",
    "    y_predict2 = y_predict2.reshape(y_test.shape)\n",
    "    y_predict3 = model3.predict(X_test)\n",
    "    y_predict3 = y_predict3.reshape(y_test.shape)\n",
    "    y_predict4 = model4.predict(X_test)\n",
    "    y_predict4 = y_predict4.reshape(y_test.shape)\n",
    "    coef1 = np.corrcoef(y_test, y_predict1)[0,1] ** 2\n",
    "    coef2 = np.corrcoef(y_test, y_predict2)[0,1] ** 2\n",
    "    coef3 = np.corrcoef(y_test, y_predict3)[0,1] ** 2\n",
    "    coef4 = np.corrcoef(y_test, y_predict4)[0,1] ** 2\n",
    "    eval_1 = model1.evaluate(X_test, y_test)\n",
    "    eval_2 = model2.evaluate(X_test, y_test)\n",
    "    eval_3 = model3.evaluate(X_test, y_test)\n",
    "    eval_4 = model4.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Results\n",
    "    result = {'Model': [name_str]*16,\n",
    "              'Train_site': [site_1] * 4 + [site_2] * 4 + [site_3] * 4 + [f'{site_2}, {site_3}'] * 4,\n",
    "              'Test_site': [site_1] * 16,\n",
    "              'Metric': ['MSE', 'RMSE', 'MAE', 'coef'] * 4,\n",
    "              'Value': [eval_1[1], eval_1[2], eval_1[3], coef1,\n",
    "                        eval_2[1], eval_2[2], eval_2[3], coef2,\n",
    "                        eval_3[1], eval_3[2], eval_3[3], coef3,\n",
    "                        eval_4[1], eval_4[2], eval_3[3], coef4]}\n",
    "    df = pd.DataFrame(result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resnet1 = cross_site(cnn_resnet, 'cnn_resnet', 0.01, 0, 3, 4, X, y)\n",
    "df_resnet2 = cross_site(cnn_resnet, 'cnn_resnet', 0.01, 3, 0, 4, X, y)\n",
    "df_resnet3 = cross_site(cnn_resnet, 'cnn_resnet', 0.01, 4, 0, 3, X, y)\n",
    "df_resnet = (df_resnet1.merge(df_resnet2, how='outer')).merge(df_resnet3, how='outer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
