{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8734f6",
   "metadata": {
    "gather": {
     "logged": 1652212959976
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import afqinsight.nn.tf_models as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from afqinsight.datasets import AFQDataset\n",
    "from afqinsight.nn.tf_models import cnn_lenet, mlp4, cnn_vgg, lstm1v0, lstm1, lstm2, blstm1, blstm2, lstm_fcn, cnn_resnet\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os.path\n",
    "# Harmonization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurocombat_sklearn import CombatModel\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, resample\n",
    "from afqinsight.augmentation import jitter, time_warp, scaling, magnitude_warp, window_warp\n",
    "import tempfile\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eccb79",
   "metadata": {
    "gather": {
     "logged": 1652212986960
    }
   },
   "outputs": [],
   "source": [
    "afq_dataset = AFQDataset.from_files(\n",
    "    fn_nodes=\"../data/raw/combined_tract_profiles.csv\",\n",
    "    fn_subjects=\"../data/raw/participants_updated_id.csv\",\n",
    "    dwi_metrics=[\"dki_fa\", \"dki_md\", \"dki_mk\"],\n",
    "    index_col=\"subject_id\",\n",
    "    target_cols=[\"age\", \"dl_qc_score\", \"scan_site_id\"],\n",
    "    label_encode_cols=[\"scan_site_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc97db2",
   "metadata": {
    "gather": {
     "logged": 1652212987198
    }
   },
   "outputs": [],
   "source": [
    "afq_dataset.drop_target_na()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ebb60",
   "metadata": {
    "gather": {
     "logged": 1652212987403
    }
   },
   "outputs": [],
   "source": [
    "print(len(afq_dataset.subjects))\n",
    "print(afq_dataset.X.shape)\n",
    "print(afq_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c366051",
   "metadata": {
    "gather": {
     "logged": 1652212988137
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = list(afq_dataset.as_tensorflow_dataset().as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fc684-8d26-435c-a63f-1554600881a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff = tf.data.Dataset.from_tensor_slices(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d75581",
   "metadata": {
    "gather": {
     "logged": 1652212988321
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([xx[0][None] for xx in full_dataset], 0)\n",
    "y = np.array([yy[1][0] for yy in full_dataset])\n",
    "qc = np.array([yy[1][1] for yy in full_dataset])\n",
    "site = np.array([yy[1][2] for yy in full_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623180c2",
   "metadata": {
    "gather": {
     "logged": 1652212988490
    }
   },
   "outputs": [],
   "source": [
    "X = X[qc>0]\n",
    "y = y[qc>0]\n",
    "site = site[qc>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86789a05-6d17-4c98-8604-b515c9c8a1dc",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1652212988730
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c33a0",
   "metadata": {
    "gather": {
     "logged": 1652212988908
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "# EarlyStopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    mode=\"min\",\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69834832-d894-4c9c-a58c-55fa2aaf6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X[site==0]\n",
    "y0 = y[site==0]\n",
    "X3 = X[site==3]\n",
    "y3 = y[site==3]\n",
    "X4 = X[site==4]\n",
    "y4 = y[site==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f4426-aab4-4069-a6ca-4e423b33610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_aug(X_in, scaler=1/20):\n",
    "    X_out = np.zeros_like(X_in)\n",
    "    for channel in range(X_in.shape[-1]):\n",
    "        this_X = X_in[..., channel][np.newaxis, ..., np.newaxis]\n",
    "        scale = np.abs(np.max(this_X) - np.min(this_X)) * scaler\n",
    "        this_X = jitter(this_X, sigma=scale)\n",
    "        this_X = scaling(this_X, sigma=scale)\n",
    "        this_X = time_warp(this_X, sigma=scale)\n",
    "        this_X = window_warp(this_X)\n",
    "        X_out[..., channel] = this_X[0, ..., 0]\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6bd91-2a69-41bb-b1dd-dacba34af0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_aug(X_in, scaler=1/25.):\n",
    "#     X_out = np.zeros_like(X_in)\n",
    "#     for channel in range(X_in.shape[-1]):\n",
    "#         this_X = X_in[..., channel][np.newaxis, ..., np.newaxis]\n",
    "#         scale = np.abs(np.max(this_X) - np.min(this_X)) * scaler\n",
    "#         this_X = jitter(this_X, sigma=scale)\n",
    "#         this_X = scaling(this_X, sigma=scale)\n",
    "#         this_X = time_warp(this_X, sigma=scale)\n",
    "#         X_out[..., channel] = this_X[0, ..., 0]\n",
    "#     return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c2290-82a3-43d5-9c75-43a74104d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_this(X_in, y_in):\n",
    "    X_out = tf.numpy_function(tf_aug, [X_in], tf.float32)    \n",
    "    return X_out, y_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47879b42-76cc-4ff5-9c3e-ce047d25cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model_func, X_train, y_train):\n",
    "    batch_size = 32\n",
    "    # Split into train and validation:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_val.astype(np.float32), y_val.astype(np.float32)))\n",
    "\n",
    "    model = model_func(input_shape=X_train.shape[1:], n_classes=1, output_activation=None, verbose=True)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['mean_squared_error', \n",
    "                           tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                           'mean_absolute_error'],\n",
    "                 )\n",
    "\n",
    "    # ModelCheckpoint\n",
    "    ckpt_filepath = tempfile.NamedTemporaryFile().name + '.h5'\n",
    "    ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = ckpt_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"auto\",\n",
    "        )\n",
    "\n",
    "    callbacks = [early_stopping, ckpt, reduce_lr]        \n",
    "    train_dataset = train_dataset.map(augment_this) \n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    history = model.fit(train_dataset, epochs=n_epochs, validation_data=val_dataset,\n",
    "                        callbacks=callbacks, use_multiprocessing=True)\n",
    "    model.load_weights(ckpt_filepath)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea47010",
   "metadata": {
    "gather": {
     "logged": 1652212989278
    }
   },
   "outputs": [],
   "source": [
    "def cross_site(model_func, name_str, lr, X, y, random_states):\n",
    "    # Split the data by sites\n",
    "    X0 = X[site==0]\n",
    "    y0 = y[site==0]\n",
    "    X3 = X[site==3]\n",
    "    y3 = y[site==3]\n",
    "    X4 = X[site==4]\n",
    "    y4 = y[site==4]\n",
    "\n",
    "    # We downsample each site down to the size of the smallest site:\n",
    "    sample_size = X4.shape[0]\n",
    "    X0, y0 = resample(X0, y0, n_samples=sample_size, replace=False, random_state=random_states[0])\n",
    "    X3, y3 = resample(X3, y3, n_samples=sample_size, replace=False, random_state=random_states[1])\n",
    "    X4, y4 = resample(X4, y4, n_samples=sample_size, replace=False, random_state=random_states[2])\n",
    "    \n",
    "    # Split the data into train and test sets:\n",
    "    X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=random_states[0])\n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=random_states[1])\n",
    "    \n",
    "    X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=random_states[2])\n",
    "    \n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    # Impute train and test separately:\n",
    "    X0_train = np.concatenate([imputer.fit_transform(X0_train[..., ii])[:, :, None] for ii in range(X0_train.shape[-1])], -1)\n",
    "    X0_test = np.concatenate([imputer.fit_transform(X0_test[..., ii])[:, :, None] for ii in range(X0_test.shape[-1])], -1)\n",
    "    X3_train = np.concatenate([imputer.fit_transform(X3_train[..., ii])[:, :, None] for ii in range(X3_train.shape[-1])], -1)\n",
    "    X3_test = np.concatenate([imputer.fit_transform(X3_test[..., ii])[:, :, None] for ii in range(X3_test.shape[-1])], -1)\n",
    "    X4_train = np.concatenate([imputer.fit_transform(X4_train[..., ii])[:, :, None] for ii in range(X4_train.shape[-1])], -1)\n",
    "    X4_test = np.concatenate([imputer.fit_transform(X4_test[..., ii])[:, :, None] for ii in range(X4_test.shape[-1])], -1)\n",
    "        \n",
    "    train_data = {0: [X0_train, y0_train], \n",
    "                  3: [X3_train, y3_train],\n",
    "                  4: [X4_train, y4_train]}\n",
    "\n",
    "    test_data = {0: [X0_test, y0_test], \n",
    "                 3: [X3_test, y3_test],\n",
    "                 4: [X4_test, y4_test]}\n",
    "\n",
    "    train_site = []\n",
    "    test_site = []\n",
    "    metric = []\n",
    "    value = []\n",
    "\n",
    "    # Train on each one separately and test on all of them\n",
    "    for train in train_data: \n",
    "        X_train, y_train = train_data[train]\n",
    "        trained = model_fit(model_func, X_train, y_train)\n",
    "        for test in test_data:\n",
    "            X_test, y_test = test_data[test]\n",
    "            y_pred = trained.predict(X_test)\n",
    "            train_site.append([train]*3)\n",
    "            test_site.append([test]*3)\n",
    "            metric.append(\"mae\")\n",
    "            value.append(mean_absolute_error(y_test, y_pred))\n",
    "            metric.append(\"mad\")\n",
    "            value.append(median_absolute_error(y_test, y_pred))\n",
    "            metric.append(\"r2\")\n",
    "            value.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    result = {'Model': [name_str] * 27,\n",
    "              'Train_site': np.array(train_site).ravel(),\n",
    "              'Test_site': np.array(test_site).ravel(),\n",
    "              'Metric': metric,\n",
    "              'Value': value}\n",
    "    df = pd.DataFrame(result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790377c-9b35-4c0c-bb97-fcfc3b5d4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "              \"cnn_lenet\": {\"model\": cnn_lenet, \"lr\": 0.001}, \n",
    "              # \"mlp4\": {\"model\": mlp4, \"lr\": 0.001},\n",
    "              # \"cnn_vgg\": {\"model\": cnn_vgg, \"lr\": 0.001},\n",
    "              # \"lstm1v0\": {\"model\": lstm1v0, \"lr\": 0.01},\n",
    "              # \"lstm1\": {\"model\": lstm1, \"lr\": 0.01},\n",
    "              # \"lstm2\": {\"model\": lstm2, \"lr\": 0.01},\n",
    "              # \"blstm1\": {\"model\": blstm1, \"lr\": 0.01},\n",
    "              # \"blstm2\": {\"model\": blstm1, \"lr\": 0.01},\n",
    "              # \"lstm_fcn\": {\"model\": lstm_fcn, \"lr\": 0.01},\n",
    "              \"cnn_resnet\": {\"model\": cnn_resnet, \"lr\": 0.01}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e465bf-19e0-4acf-b687-6140186a1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f7013-3446-463e-855d-a2a434b69e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = np.abs(np.floor(np.random.randn(3 * n_runs )*1000)).astype(int).reshape((n_runs, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_name in model_dict:\n",
    "    model_func = model_dict[model_name][\"model\"]\n",
    "    lr = model_dict[model_name][\"lr\"]\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model_name)\n",
    "    for ii in range(n_runs):     \n",
    "        dfs.append(cross_site(model_func, model_name, lr, X, y, random_states[ii]))\n",
    "        one_df = pd.concat(dfs)\n",
    "        one_df.to_csv(\"cross_site_online_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9bc2f-eea3-426a-8929-6858e1529834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_pt_tf"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "azureml_py38_pt_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
