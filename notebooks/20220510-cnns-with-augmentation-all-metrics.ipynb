{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8734f6",
   "metadata": {
    "gather": {
     "logged": 1652119071245
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'afqinsight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1a43a9b9216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mafqinsight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_models\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mafqinsight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAFQDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'afqinsight'"
     ]
    }
   ],
   "source": [
    "import afqinsight.nn.tf_models as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from afqinsight.datasets import AFQDataset\n",
    "from afqinsight.nn.tf_models import cnn_lenet, mlp4, cnn_vgg, lstm1v0, lstm1, lstm2, blstm1, blstm2, lstm_fcn, cnn_resnet\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os.path\n",
    "# Harmonization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurocombat_sklearn import CombatModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eccb79",
   "metadata": {
    "gather": {
     "logged": 1652119099910
    }
   },
   "outputs": [],
   "source": [
    "afq_dataset = AFQDataset.from_files(\n",
    "    fn_nodes=\"../data/raw/combined_tract_profiles.csv\",\n",
    "    fn_subjects=\"../data/raw/participants_updated_id.csv\",\n",
    "    dwi_metrics=[\"dki_fa\", \"dki_md\", \"dki_mk\"],\n",
    "    index_col=\"subject_id\",\n",
    "    target_cols=[\"age\", \"dl_qc_score\", \"scan_site_id\"],\n",
    "    label_encode_cols=[\"scan_site_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc97db2",
   "metadata": {
    "gather": {
     "logged": 1652119100074
    }
   },
   "outputs": [],
   "source": [
    "afq_dataset.drop_target_na()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ebb60",
   "metadata": {
    "gather": {
     "logged": 1652119100214
    }
   },
   "outputs": [],
   "source": [
    "print(len(afq_dataset.subjects))\n",
    "print(afq_dataset.X.shape)\n",
    "print(afq_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c366051",
   "metadata": {
    "gather": {
     "logged": 1652119101512
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = list(afq_dataset.as_tensorflow_dataset().as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2e7ff",
   "metadata": {
    "gather": {
     "logged": 1652119101663
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([xx[0][None] for xx in full_dataset], 0)\n",
    "y = np.array([yy[1][0] for yy in full_dataset])\n",
    "qc = np.array([yy[1][1] for yy in full_dataset])\n",
    "site = np.array([yy[1][2] for yy in full_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b259a0",
   "metadata": {
    "gather": {
     "logged": 1652119101816
    }
   },
   "outputs": [],
   "source": [
    "X = X[qc>0]\n",
    "y = y[qc>0]\n",
    "site = site[qc>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79d7ce",
   "metadata": {
    "gather": {
     "logged": 1652119102101
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test, site_train, site_test = train_test_split(X, y, site, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966984fd",
   "metadata": {
    "gather": {
     "logged": 1652119102266
    }
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854bed1",
   "metadata": {
    "gather": {
     "logged": 1652119102440
    }
   },
   "outputs": [],
   "source": [
    "# Impute train and test separately:\n",
    "X_train = np.concatenate([imputer.fit_transform(X_train[..., ii])[:, :, None] for ii in range(X_train.shape[-1])], -1)\n",
    "X_test = np.concatenate([imputer.fit_transform(X_test[..., ii])[:, :, None] for ii in range(X_test.shape[-1])], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f18649",
   "metadata": {
    "gather": {
     "logged": 1652119103952
    }
   },
   "outputs": [],
   "source": [
    "# Combat\n",
    "X_train = np.concatenate([CombatModel().fit_transform(X_train[..., ii], site_train[:, None], None, None)[:, :, None] for ii in range(X_train.shape[-1])], -1)\n",
    "X_test = np.concatenate([CombatModel().fit_transform(X_test[..., ii], site_test[:, None], None, None)[:, :, None] for ii in range(X_test.shape[-1])], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2e9ca",
   "metadata": {
    "gather": {
     "logged": 1652119104214
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "# EarlyStopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    mode=\"min\",\n",
    "    patience=100\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3fa6a",
   "metadata": {
    "gather": {
     "logged": 1652119104392
    }
   },
   "outputs": [],
   "source": [
    "from afqinsight.augmentation import jitter, time_warp, scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb58b1b",
   "metadata": {
    "gather": {
     "logged": 1652119104562
    }
   },
   "outputs": [],
   "source": [
    "def augment_this(X, y, rounds=2): \n",
    "    new_X = X[:]\n",
    "    new_y = y[:]\n",
    "    for f in range(rounds): \n",
    "        aug_X = np.zeros_like(X)\n",
    "        # Do each channel separately:\n",
    "        for channel in range(aug_X.shape[-1]):\n",
    "            this_X = X[..., channel][..., np.newaxis]\n",
    "            this_X = jitter(this_X, sigma=np.mean(this_X)/25)\n",
    "            this_X = scaling(this_X, sigma=np.mean(this_X)/25)\n",
    "            this_X = time_warp(this_X, sigma=np.mean(this_X)/25)\n",
    "            aug_X[..., channel] = this_X[...,0]\n",
    "        new_X = np.concatenate([new_X, aug_X])\n",
    "        new_y = np.concatenate([new_y, y])\n",
    "    return new_X, new_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffc58d-e5bf-47e3-a82f-94df7f92ee56",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1652119104750
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle, resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf713d5",
   "metadata": {
    "gather": {
     "logged": 1652119104978
    }
   },
   "outputs": [],
   "source": [
    "# Generate evaluation results, training history, number of epochs\n",
    "def model_history(model_name, ckpt_filepath, lr, X_train, y_train):\n",
    "    model = model_name(input_shape=(100, 24), n_classes=1, output_activation=None, verbose=True)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['mean_squared_error', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mean_absolute_error'])\n",
    "    # ModelCheckpoint\n",
    "    ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    "    )\n",
    "\n",
    "    # CSVLogger\n",
    "    log = tf.keras.callbacks.CSVLogger(filename= str(model_name) + '.csv', append=True)\n",
    "    callbacks = [early_stopping, ckpt, reduce_lr, log]\n",
    "    # Augment\n",
    "    X_train, y_train = augment_this(X_train, y_train, rounds=6)\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=128, validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "    model.load_weights(ckpt_filepath)\n",
    "    eval_model = model.evaluate(X_test, y_test)\n",
    "    count_epochs = history.epoch[-1]+1\n",
    "    return eval_model, history, count_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa3e87",
   "metadata": {
    "gather": {
     "logged": 1652119105194
    }
   },
   "outputs": [],
   "source": [
    "# Visualization of mean_squared_error, root_mean_squared_error, and mean_absolute_error\n",
    "def vis_results(model_name, history, epoch):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=[20,5])\n",
    "    fig.suptitle(model_name + ' epoch = ' + str(epoch), fontsize=15)\n",
    "    ax[0].plot(history.history['loss'][10:])\n",
    "    ax[0].plot(history.history['val_loss'][10:])\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].set_title('Mean Squared Error')\n",
    "    ax[0].legend(['train', 'val'], loc='upper right')\n",
    "    ax[1].plot(history.history['rmse'][10:])\n",
    "    ax[1].plot(history.history['val_rmse'][10:])\n",
    "    ax[1].set_ylabel('roor_mean_squared_error')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_title('Root Mean Squared Error')\n",
    "    ax[1].legend(['train', 'val'], loc='upper right')\n",
    "    ax[2].plot(history.history['mean_absolute_error'][10:])\n",
    "    ax[2].plot(history.history['val_mean_absolute_error'][10:])\n",
    "    ax[2].set_ylabel('mean_absolute_error')\n",
    "    ax[2].set_xlabel('epoch')\n",
    "    ax[2].set_title('Mean Absolute Error')\n",
    "    ax[2].legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ad810-a944-44f7-a5cb-c1b5b9859f65",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1652119105508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ce224-53e2-496b-9135-791d99187b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\"cnn_lenet\": {\"model\": cnn_lenet, \"lr\": 0.001}, \n",
    "              \"mlp4\": {\"model\": mlp4, \"lr\": 0.001},\n",
    "              \"cnn_vgg\": {\"model\": cnn_vgg, \"lr\": 0.001},\n",
    "              \"lstm1v0\": {\"model\": lstm1v0, \"lr\": 0.01},\n",
    "              \"lstm1\": {\"model\": lstm1, \"lr\": 0.01},\n",
    "              \"lstm2\": {\"model\": lstm2, \"lr\": 0.01},\n",
    "              \"blstm1\": {\"model\": blstm1, \"lr\": 0.01},\n",
    "              \"blstm2\": {\"model\": blstm1, \"lr\": 0.01},\n",
    "              \"lstm_fcn\": {\"model\": lstm_fcn, \"lr\": 0.01},\n",
    "              \"cnn_resnet\": {\"model\": cnn_resnet, \"lr\": 0.01}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6eb583-8ec7-4b42-a484-9539de46d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_dict: \n",
    "    print(model_dict[model][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fe4da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### cnn_lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a4145-6070-417f-b859-4f001cef9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37d42a-876e-4fb4-abed-f8ab7b784698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd035f48",
   "metadata": {
    "gather": {
     "logged": 1652213549742
    }
   },
   "outputs": [],
   "source": [
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    results[model] = []\n",
    "    history[model] = {\"mean_absolute_error\": [],\n",
    "                      \"val_mean_absolute_error\": []}\n",
    "    for ii in range(10): \n",
    "        this_eval, this_history, this_epochs = model_history(model_dict[model][\"model\"],\n",
    "                                                             tempfile.NamedTemporaryFile().name + \".h5\", \n",
    "                                                             model_dict[model][\"lr\"], \n",
    "                                                             X_train, \n",
    "                                                             y_train)\n",
    "        results[model].append(this_eval[1:])\n",
    "        history[model][\"mean_absolute_error\"].append(this_history.history['mean_absolute_error'])\n",
    "        history[model][\"val_mean_absolute_error\"].append(this_history.history['val_mean_absolute_error'])\n",
    "        with open(f'results_{model}_all_metrics.pickle', 'wb') as file:\n",
    "            pickle.dump(results[model], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f'history_{model}_all_metrics.pickle', 'wb') as file:\n",
    "            pickle.dump(history[model], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc618f-17b2-4498-a1b3-4658fa28eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_pt_tf"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "azureml_py38_pt_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
