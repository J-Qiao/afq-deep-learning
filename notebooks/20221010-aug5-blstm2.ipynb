{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8734f6",
   "metadata": {
    "gather": {
     "logged": 1652119071245
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import afqinsight.nn.tf_models as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from afqinsight.datasets import AFQDataset\n",
    "from afqinsight.nn.tf_models import cnn_lenet, mlp4, cnn_vgg, lstm1v0, lstm1, lstm2, blstm1, blstm2, lstm_fcn, cnn_resnet\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os.path\n",
    "# Harmonization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurocombat_sklearn import CombatModel\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "\n",
    "import pickle\n",
    "import tools\n",
    "from tools import load_data, model_fit, fit_and_eval, augment_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa706a12-a3cb-4090-8737-a2b420e19e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 21:09:49.511224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:49.518256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:49.518638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:49.519666: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-12 21:09:49.520047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:49.520427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:49.520791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:50.128937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:50.129227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:50.129484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 21:09:50.129670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6927 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2\n"
     ]
    }
   ],
   "source": [
    "X, y, site = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889f9367-59a8-4fd8-a516-5036c862eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "#   \"cnn_lenet\": {\"model\": cnn_lenet, \"lr\": 0.001}, \n",
    "  # \"mlp4\": {\"model\": mlp4, \"lr\": 0.001},\n",
    "  # \"cnn_vgg\": {\"model\": cnn_vgg, \"lr\": 0.001},\n",
    "  # \"lstm1v0\": {\"model\": lstm1v0, \"lr\": 0.01},\n",
    "  # \"lstm1\": {\"model\": lstm1, \"lr\": 0.01},\n",
    "  # \"lstm2\": {\"model\": lstm2, \"lr\": 0.01},\n",
    "  # \"blstm1\": {\"model\": blstm1, \"lr\": 0.01},\n",
    "  \"blstm2\": {\"model\": blstm2, \"lr\": 0.01},\n",
    "  # \"lstm_fcn\": {\"model\": lstm_fcn, \"lr\": 0.01},\n",
    "  # \"cnn_resnet\": {\"model\": cnn_resnet, \"lr\": 0.01}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626db868-686b-49f5-9a02-638130f76687",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cc071c-aec4-4786-9dd3-932697bf5352",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale = 1/5 \n",
    "# scale = 1/10\n",
    "# scale = 1/20 \n",
    "# scale = 1/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4f3387-bb0c-4b6c-ac9a-8f6e87be0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated once with this code and then hard-coded:\n",
    "# seeds = np.array([np.abs(np.floor(np.random.randn()*1000)) for ii in range(n_runs)], dtype=int)\n",
    "\n",
    "seeds = np.array([484, 645, 714, 244, 215, 1503, 1334, 1576, 469, 1795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb566ad-94c2-4a5d-9683-f0588b4a8eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "model:  blstm2\n",
      "run:  0\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 72)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 200)          138400    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 379,401\n",
      "Trainable params: 379,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 21:09:56.959787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-12 21:10:03.298068: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 49s 1s/step - loss: 16.0191 - mean_squared_error: 16.0191 - rmse: 4.0024 - mean_absolute_error: 3.1448 - val_loss: 11.8805 - val_mean_squared_error: 11.8805 - val_rmse: 3.4468 - val_mean_absolute_error: 2.9009\n",
      "Epoch 2/1000\n",
      " 6/37 [===>..........................] - ETA: 36s - loss: 10.9510 - mean_squared_error: 10.9510 - rmse: 3.3092 - mean_absolute_error: 2.6991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_runs): \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ii)\n\u001b[0;32m----> 8\u001b[0m     this_eval, this_pred \u001b[38;5;241m=\u001b[39m \u001b[43mfit_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43msite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment_this\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     this_eval[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ii\n\u001b[1;32m     18\u001b[0m     this_pred[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ii\n",
      "File \u001b[0;32m~/work/afq-deep-learning/notebooks/tools.py:133\u001b[0m, in \u001b[0;36mfit_and_eval\u001b[0;34m(model, model_dict, X, y, site, random_state, batch_size, n_epochs, augment, train_size)\u001b[0m\n\u001b[1;32m    130\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([CombatModel()\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ii], site_train[:, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    131\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([CombatModel()\u001b[38;5;241m.\u001b[39mfit_transform(X_test[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ii], site_test[:, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m trained \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m metric \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    137\u001b[0m value \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/work/afq-deep-learning/notebooks/tools.py:109\u001b[0m, in \u001b[0;36mmodel_fit\u001b[0;34m(model_func, X_train, y_train, lr, batch_size, n_epochs, augment)\u001b[0m\n\u001b[1;32m    107\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m    108\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[0;32m--> 109\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(ckpt_filepath)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1189\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1191\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 435\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:315\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    312\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    318\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:353\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    352\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 353\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    356\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:1028\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1028\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:1100\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1099\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/tf_utils.py:516\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/tf_utils.py:512\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    511\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 512\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model, \n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=None,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_1_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_1_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd035f48",
   "metadata": {
    "gather": {
     "logged": 1652213549742
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model,\n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=1000,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_2_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_2_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12595999-03fa-484d-a7e4-22608ba7605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model, \n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=700,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_3_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_3_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac67b58-d389-4cf3-8588-cee8663dd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model, \n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=350,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_4_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_4_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368ad9a-1e21-4cfe-ac05-ef6c2f8c5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model,\n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=175,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_5_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_5_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfc0aa-e955-4b0e-9a0e-e876a983283c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs_eval = []\n",
    "dfs_pred = []\n",
    "for model in model_dict:\n",
    "    print(\"##################################################\")\n",
    "    print(\"model: \", model)\n",
    "    for ii in range(n_runs): \n",
    "        print(\"run: \", ii)\n",
    "        this_eval, this_pred = fit_and_eval(\n",
    "            model, \n",
    "            model_dict,\n",
    "            X, \n",
    "            y, \n",
    "            site,\n",
    "            random_state=seeds[ii],\n",
    "            train_size=100,\n",
    "            augment=augment_this)\n",
    "        this_eval[\"run\"] = ii\n",
    "        this_pred[\"run\"] = ii\n",
    "        dfs_eval.append(this_eval)\n",
    "        dfs_pred.append(this_pred)\n",
    "        # Save evaluation metrics\n",
    "        one_df = pd.concat(dfs_eval)\n",
    "        one_df.to_csv(\"blstm2_aug5_6_eval.csv\")\n",
    "        # Save predictions and test values:\n",
    "        one_df = pd.concat(dfs_pred)\n",
    "        one_df.to_csv(\"blstm2_aug5_6_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda77c02-1188-4abe-8b17-9e16d7efc495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_pt_tf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
